（一次的な内容です）
## KVキャッシュ（Key-Value Cache）

### なぜキャッシュが有効なのか？

Transformerの自己注意機構では、各トークン生成時に**過去のすべてのトークンとの注意計算**が必要です。キャッシュなしだと：

- 1トークン目：1個のトークンで計算
- 2トークン目：2個のトークンで計算（1個目を再計算）
- 3トークン目：3個のトークンで計算（1-2個目を再計算）
- n トークン目：n個のトークンで計算

→ 計算量がO(n²)になり、非効率的

### 何をキャッシュし、何を更新するか？

**キャッシュするもの：**
- 過去に生成した全トークンの**KeyとValue**の行列
- 各レイヤーごとに保存

**毎回更新するもの：**
- 新しく生成したトークン1つ分のK, Vを**追加**
- 新しいトークンのQuery（Q）を計算し、キャッシュされたK, V全体と注意計算

```
ステップ t:
  新しいQ_t を計算
  K_cache = [K_1, K_2, ..., K_{t-1}]  # 既存
  V_cache = [V_1, V_2, ..., V_{t-1}]  # 既存
  
  Attention(Q_t, K_cache, V_cache) を計算
  
  K_t, V_t を計算してキャッシュに追加
  K_cache = [K_1, K_2, ..., K_{t-1}, K_t]
  V_cache = [V_1, V_2, ..., V_{t-1}, V_t]
```

### Encoder側？Decoder側？

**主にDecoder側で使用：**
- GPTなどの自己回帰モデル（デコーダーのみ）
- Decoder-only architectureでの逐次生成時

**Encoderでは通常不要：**
- 入力全体を一度に処理（並列計算）
- 逐次生成がないため

**Encoder-Decoderモデル（T5、BARTなど）の場合：**
- Encoderの出力（K, V）は1回計算して全デコードステップで再利用
- Decoderの自己注意部分でKVキャッシュを使用

### メリット
- 計算量：O(n²) → O(n)に削減
- 生成速度が大幅に向上
- メモリ使用量とのトレードオフ

KVキャッシュは長文生成を実用的な速度で実現する重要な最適化技術です。
