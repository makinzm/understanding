# Meta Information

- URL: [SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code](https://arxiv.org/abs/2602.11209)
- LICENSE: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) / [arXiv.org - Non-exclusive license to distribute](https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html)
- Reference: Ziyi Yang, Kalit Inani, Keshav Kabra, Vima Gupta, Anand Padmanabha Iyer (2026). SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code. arXiv:2602.11209 [cs.SE].

---

# SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code

## Background and Motivation

As LLMs (e.g., Claude, GitHub Copilot, Qwen) are increasingly used to generate code in production settings, ensuring the correctness and reliability of such code becomes critical. LLM-generated code frequently contains algorithmic vulnerabilities—subtle logical errors and edge-case failures that differ from traditional memory-safety bugs targeted by conventional fuzzing.

Traditional fuzzing tools such as AFL, LibFuzzer, and Jazzer allocate testing resources uniformly and lack awareness of the semantic structure of algorithmic code. This leads to two problems:

1. **Inefficient resource allocation**: Resources are wasted on code paths with low vulnerability potential.
2. **Missed vulnerabilities**: Algorithmic bugs triggered only under semantically meaningful input conditions are not reached.

> [!NOTE]
> The paper defines "algorithmic vulnerabilities" as logical errors in LLM-generated algorithm implementations—distinct from memory corruption bugs targeted by classical fuzzers.

SAFuzz is proposed to address these gaps through a hybrid framework combining semantic guidance and adaptive resource management.

---

## Problem Formulation

Given a natural language programming problem $P$ and a code solution $C$ generated by an LLM, SAFuzz aims to:

- **Input**: Problem description $P$, LLM-generated code $C$, a reference oracle (correct implementation or test case suite)
- **Output**: A set of test inputs $T^* = \{t_1, t_2, \ldots, t_k\}$ that expose incorrect behavior in $C$ (i.e., outputs that differ from the oracle)

The target domain is **competitive programming / algorithmic problems** (e.g., from CSES), where a ground-truth oracle can be derived from problem constraints or a correct reference implementation.

---

## Methodology: SAFuzz Framework

SAFuzz consists of three main components:

### 1. Prompt-Based Behavioral Diversification

To explore the behavioral space of the LLM-generated code, SAFuzz uses a secondary LLM to generate semantically diverse test inputs. Rather than random mutations, the system constructs structured prompts that:

- Describe the algorithmic problem $P$
- Request inputs targeting specific boundary cases (e.g., maximum/minimum values, degenerate graph structures, empty inputs)
- Generate inputs across multiple behavioral categories identified from the problem semantics

This produces a diverse initial seed corpus $S = \{s_1, \ldots, s_n\}$ where each seed $s_i$ targets a distinct semantic execution path.

> [!IMPORTANT]
> Unlike random fuzzing, behavioral diversification is **problem-aware**: the LLM understands the problem statement and generates inputs that are semantically meaningful (e.g., for a graph problem, it generates disconnected graphs, cycles, complete graphs, etc.).

### 2. Harness Generation with Problem-Specific Oracles

SAFuzz automatically generates a fuzzing harness for each problem:

```
ALGORITHM: Harness Generation
Input: Problem P, LLM code C, reference implementation R
Output: Harness H that runs C on input t and compares to R(t)

1. Parse problem constraints from P to determine input format
2. Generate input parser to decode raw bytes into structured input t
3. Wrap C in a function call: actual_output = C(t)
4. Compute expected_output = R(t) using reference oracle
5. If actual_output ≠ expected_output → report as bug
6. Return harness H = (input_parser, C_wrapper, oracle_comparator)
```

The oracle $R$ can be:
- A correct reference solution (when available)
- Algebraic constraints derived from problem specification (e.g., output must be ≤ input size)
- Unit tests provided in the problem statement

### 3. LLM-Based Predictor for Adaptive Early Stopping

The key innovation for resource efficiency is a trained predictor $f_\theta$ that estimates the **bug-finding potential** of a test input $t$ given the current fuzzing state.

```
ALGORITHM: Adaptive Fuzzing with Early Stopping
Input: Seed corpus S, harness H, budget B (total CPU time)
Output: Bug-triggering inputs T*

1. Initialize queue Q = S, budget_used = 0, T* = {}
2. While budget_used < B:
   a. Pick next seed s from Q (by coverage priority)
   b. Mutate s to generate candidate inputs {c_1, ..., c_m}
   c. For each candidate c_i:
      - Compute feature vector φ(c_i) from c_i and execution trace
      - Score p_i = f_θ(φ(c_i))  ← predicted bug probability
      - If p_i < threshold τ: skip (early stopping)
      - Else: run H(c_i), record result, update Q if new coverage
   d. budget_used += time_elapsed
3. Return T*
```

The predictor $f_\theta$ is trained on (input features, bug-found label) pairs collected from prior fuzzing runs. Features $\phi(c_i)$ include:
- Input structural features (size, value ranges, structural properties)
- Coverage feedback (new branches reached by $c_i$)
- Semantic similarity to previously bug-triggering inputs

> [!NOTE]
> Dynamic early stopping avoids executing low-potential inputs, achieving a **1.71× speedup** over GreenFuzz (the SOTA efficiency-focused fuzzer) without sacrificing recall.

---

## Integration with Existing Fuzzers

SAFuzz sits on top of existing coverage-guided fuzzers (AFL, LibFuzzer, Jazzer) and enhances them with:

| Component | Role |
|-----------|------|
| Behavioral diversification | Replaces random initial seeds with semantically targeted seeds |
| Harness generation | Automates test oracle creation (no manual effort) |
| LLM-based predictor | Replaces uniform resource allocation with adaptive prioritization |

> [!TIP]
> SAFuzz is complementary to unit test generation tools. When combined with unit test generation, bug detection recall improves from 67.3% to 79.5%.

---

## Comparison with Related Methods

| Method | Semantic Awareness | Adaptive Allocation | Oracle Automation | Target |
|--------|-------------------|--------------------|--------------------|--------|
| AFL | No | No | No | Memory bugs |
| LibFuzzer | No (coverage-only) | No | No | Memory bugs |
| GreenFuzz | Partial | Yes (efficiency) | No | General code |
| **SAFuzz** | **Yes** | **Yes (LLM predictor)** | **Yes (auto harness)** | **LLM-generated code** |

Key differences from GreenFuzz:
- GreenFuzz focuses on energy/resource efficiency via coverage metrics but lacks semantic understanding of algorithmic problems.
- SAFuzz explicitly uses problem semantics (via LLM prompting) to diversify seeds and assess test quality.
- SAFuzz automates oracle generation, removing the need for manually written test harnesses.

> [!CAUTION]
> The paper targets a specific niche: algorithmic programming problems with verifiable correct outputs. Applicability to open-ended code generation tasks (e.g., web APIs, UI code) is not evaluated and may be limited due to the reliance on a ground-truth oracle.

---

# Experiments

- **Dataset**: CSES (Competitive Programmer's Handbook problem set) — algorithmic programming problems with well-defined input/output specifications and reference solutions
- **LLM Code Generators Evaluated**: Claude, GitHub Copilot, Qwen (multiple LLM-generated code solutions per problem)
- **Baselines**: GreenFuzz (SOTA efficiency-focused fuzzer), standard AFL/LibFuzzer, unit test generation tools
- **Hardware**: Not explicitly stated in the available content
- **Optimizer**: Not applicable (no gradient-based training details provided in available content)
- **Evaluation Metrics**:
  - Vulnerability discrimination precision (correct bug vs. non-bug classification)
  - Bug detection recall
  - Time cost (wall-clock time relative to GreenFuzz)

**Key Results**:

| Metric | Baseline | SAFuzz |
|--------|----------|--------|
| Precision | 77.9% | 85.7% (+7.8pp) |
| Time cost | 1.0× (GreenFuzz) | 0.58× (1.71× faster) |
| Recall (standalone) | 67.3% | comparable |
| Recall (+ unit tests) | 67.3% | 79.5% (+12.2pp) |

> [!IMPORTANT]
> The 1.71× speedup is achieved by the LLM-based early stopping predictor, which skips low-potential test candidates without running the full fuzzing harness.

---

## Applicability

- **Who**: Software engineers and QA teams using LLM-assisted code generation in workflows where correctness is critical (e.g., competitive programming judges, embedded systems, financial algorithms).
- **When**: At test time, after an LLM generates a code solution, before deployment or submission.
- **Where**: Domains with verifiable ground-truth outputs: algorithmic problems, mathematical computations, protocol implementations with formal specifications.
- **Limitations**: Requires a reference oracle (correct implementation or formal specification). Not directly applicable to subjective or open-ended code generation tasks.
