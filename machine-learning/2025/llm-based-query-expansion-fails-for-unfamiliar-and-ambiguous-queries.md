# Meta Information

- URL: [LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries](https://arxiv.org/abs/2505.12694)
- LICENSE: [arXiv.org - Non-exclusive license to distribute](https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html)
- Reference: Abe, K., Takeoka, K., Kato, M. P., & Oyamada, M. (2025). LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries. Proceedings of SIGIR '25, July 13–18, 2025, Padua, Italy.

# Overview

LLM-based query expansion (QE) augments a user query with terms generated by a language model before passing it to a retrieval system. While LLMs improve over classical rule-based and co-occurrence-based QE methods, they introduce two new failure modes:

1. **Knowledge deficiency**: The LLM generates factually wrong expansions when it lacks knowledge about the query topic, degrading retrieval precision.
2. **Ambiguity bias**: For queries with multiple valid interpretations, the LLM biases the expansion toward popular interpretations, reducing recall for rarer but equally valid meanings.

**Applicability:** Information retrieval researchers and practitioners deploying LLM-based QE in production search systems, particularly in general-domain and web-search settings. The findings inform when QE should be applied adaptively.

**Research questions:**
- **RQ1:** How does QE effectiveness differ between queries where the LLM has sufficient knowledge versus insufficient knowledge?
- **RQ2:** How does query ambiguity affect QE effectiveness?

# Background: LLM-based Query Expansion Methods

Three families of LLM-based QE are evaluated:

| Method | Type | Mechanism | Output |
|---|---|---|---|
| Q2E (Query2Expansion) | Keyword-type | Prompts LLM to generate related keywords | Keyword list appended to query |
| Q2D (Query2Doc) | Answer-type | Prompts LLM for a pseudo-answer document | Full-text pseudo document prepended |
| GaQR | Question-type | Rewrites the original query for better retrieval | Rewritten query string |

> [!NOTE]
> GaQR uses GPT-3.5-turbo as a teacher to fine-tune Llama-3-8B (student) via distillation; at inference, only the student model runs.

**Input/Output:**
- Input: query string $q \in \mathcal{V}^*$ (a sequence of tokens)
- Output: expanded query $q' \in \mathcal{V}^*$, which is passed to the first-stage retriever

**Retrieval pipeline:**
$$q \xrightarrow{\text{LLM QE}} q' \xrightarrow{\text{Retriever}} \text{ranked document list}$$

# Experimental Setup

## Datasets

### RQ1 — Knowledge Sufficiency

| Dataset | Domain | Query Type | Relevance |
|---|---|---|---|
| Natural Questions (NQ) | General (Wikipedia) | Factoid, short-answer | Entity/snippet |
| TriviaQA (TQ) | General (trivia) | Factoid, short-answer | Entity/snippet |
| MS MARCO | Web search | Mixed (factoid + procedural) | Passage-level |
| BioASQ | Biomedical (specialized) | Factoid + list | Document-level |

### RQ2 — Query Ambiguity

| Dataset | Basis | Low-ambiguity | High-ambiguity |
|---|---|---|---|
| AmbigDocs | Wikipedia entity disambiguation | 2–3 interpretations | 4+ interpretations |
| AmbigQA | Wikipedia multi-type ambiguity | Single interpretation | Multiple interpretations |
| TREC Web Track 09–12 | Web search subtopics | ≤3 subtopics | 4+ subtopics |

Unjudged documents are treated as non-retrieved (condensed-list evaluation) to maintain test-collection reusability.

## Retrieval Models

- **BM25** (sparse): $k_1 = 0.9$, $b = 0.4$
- **Contriever** (dense): bi-encoder, trained with contrastive learning
- **E5-base-v2** (dense): limited to 256-token input

## LLMs

- GPT-3.5-turbo-0125 (OpenAI)
- Meta-Llama-3-8B-Instruct

Both used in zero-shot mode.

## Knowledge Assessment Method

Whether an LLM "knows" the answer to a query is operationalized differently by query type:

**Short-answer datasets (NQ, TQ):**
$$\text{Knows}(q) = \mathbf{1}[\text{LLM response} = \text{ground-truth answer (exact match)}]$$

**Long-answer datasets (MS MARCO, BioASQ):**
$$\text{Knows}(q) = \mathbf{1}[\text{GPT-4-turbo judges response as correct or partially correct}]$$

## Evaluation Metrics

- **NDCG@10**: Normalized Discounted Cumulative Gain at rank 10 — measures relevance quality of top results
- **Recall@100**: Fraction of all relevant documents retrieved in the top 100 — measures coverage

# Results: RQ1 — Knowledge Sufficiency

## Main Finding

When the LLM lacks knowledge about a query, QE improvements in **NDCG@10 are significantly lower** in general-domain settings (NQ, TQ, MS MARCO). Specialized domains (BioASQ) are less affected because the retrieval index itself may compensate for LLM gaps.

> [!NOTE]
> The paper states: "when the LLM has sufficient knowledge, the negative impact of QE is lower than when the LLM does not."

## Effect by QE Method

| Condition | Q2D | Q2E | GaQR |
|---|---|---|---|
| LLM has knowledge | Large NDCG improvement | Moderate improvement | Moderate improvement |
| LLM lacks knowledge | Degrades performance | Degrades performance | **Maintains or improves** |

**Why GaQR is robust:** Q2D and Q2E generate factual content (answers and keywords) that directly depend on LLM knowledge — incorrect proper nouns are weighted heavily by retrieval models and hurt precision. GaQR instead **paraphrases the query** without asserting facts, making its output less sensitive to knowledge gaps.

## Effect by Query Type

Declarative queries (what/who questions) vs. procedural queries (how questions):

- **Q2D on declarative + knowledge-lacking**: pseudo-answers contain wrong proper nouns → retrieval degradation
- **Q2D on procedural + knowledge-lacking**: expansions rarely require precise proper nouns → less harmful
- **GaQR**: never falls below baseline for either query type

**Algorithmic explanation:**

For a declarative query $q$ = "Who invented X?", Q2D generates a pseudo-document $d_{\text{pseudo}}$ containing an entity name $e$. If $e \neq e_{\text{correct}}$, the retrieval model $f(q', d) = f(q + d_{\text{pseudo}}, d)$ assigns high score to documents about $e$ rather than $e_{\text{correct}}$.

For a procedural query $q$ = "How to do Y?", $d_{\text{pseudo}}$ contains steps and verbs; inaccuracies in steps do not redirect to an entirely wrong entity cluster.

# Results: RQ2 — Query Ambiguity

## Main Finding

**Recall@100 is significantly lower for high-ambiguity queries** in most settings (AmbigDocs, TREC Web Track), meaning QE fails to surface documents for rarer interpretations of the query.

> [!NOTE]
> AmbigQA results are less conclusive because only ~23% of its ambiguous queries involve entity-reference ambiguity; the remaining queries are ambiguous in answer type or time, which produces less document diversity and smaller recall gaps between ambiguity levels.

## Popularity Bias in Ambiguous Queries

The mechanism behind ambiguity failure is **popularity bias**: LLMs overrepresent common entities in their training data, so expansions of ambiguous queries reflect the most popular interpretation.

**Observed pattern (Figure 2):**
- Interpretation popularity ranked by Wikipedia page views: rank 1 = most popular
- For ranks 1–2: QE improves or does not hurt retrieval (expansion aligned with popular meaning)
- For ranks 3+: QE effectiveness degrades progressively (expansion diverges from rarer meanings)

> [!IMPORTANT]
> GaQR is an exception: it "consistently outperforms the baseline regardless of interpretation popularity" on BM25, because query rewriting does not introduce biased entity information.

**Formal view:** For an ambiguous query $q$ with interpretations $\{m_1, m_2, \ldots, m_k\}$ ordered by popularity $p(m_1) \geq p(m_2) \geq \cdots \geq p(m_k)$:

$$\text{Effectiveness}(q' \mid m_i) \propto \Pr_{\text{LLM}}(m_i \mid q) \approx p(m_i)$$

Since $p(m_1) \gg p(m_k)$ for typical Wikipedia entities, expansions implicitly commit to $m_1$, harming recall for $m_k$.

# Comparison with Related Work

| Aspect | This Work | Prior QE Analysis | Classical QE (PRF) |
|---|---|---|---|
| Failure cause studied | Knowledge gap + ambiguity | Hallucination (surface-level) | Vocabulary mismatch |
| Scale of evaluation | Large multi-dataset | Small examples | Corpus statistics |
| Ambiguity analysis | Explicit + popularity bias | Facet diversity only | Not addressed |
| Proposed solution | Adaptive QE via knowledge/ambiguity detection | None | Pseudo-relevance feedback |

> [!TIP]
> Pseudo-relevance feedback (PRF) expands using top-retrieved documents rather than LLM knowledge — see [Lavrenko & Croft, 2001] for the relevance model variant. PRF is less susceptible to LLM knowledge gaps but sensitive to initial retrieval quality.

# Conclusions and Implications

The study establishes two systematic failure modes for LLM-based QE:

1. **Knowledge gap** → incorrect factual expansions → precision loss, especially for declarative queries
2. **Popularity bias on ambiguous queries** → biased expansions toward common interpretations → recall loss for rare interpretations

**Practical implication:** LLM-based QE should be applied **adaptively**:
- Assess LLM confidence/knowledge about the query before expanding
- Detect query ambiguity (e.g., via entity linking or intent classification) and either generate diverse expansions or skip QE for highly ambiguous queries

**Future directions proposed:** chain-of-thought prompting to encourage knowledge-checking, multiple diverse expansions to cover interpretations, modules that gate QE application based on estimated query difficulty.

# Experiments

- **Datasets:** Natural Questions, TriviaQA, MS MARCO, BioASQ (RQ1); AmbigDocs, AmbigQA, TREC Web Track 09–12 (RQ2)
- **Hardware:** Not specified
- **QE LLMs:** GPT-3.5-turbo-0125, Meta-Llama-3-8B-Instruct (zero-shot)
- **Knowledge judge:** GPT-4-turbo (for MS MARCO and BioASQ long-answer evaluation)
- **Retrieval models:** BM25 ($k_1=0.9$, $b=0.4$), Contriever, E5-base-v2
- **Metrics:** NDCG@10, Recall@100; statistical significance via t-test ($p \leq 0.05$)
- **Key results:**
  - Q2D on NQ: +7.42 NDCG@10 with knowledge, substantially lower without
  - GaQR: consistently at or above baseline across knowledge and ambiguity conditions
  - High-ambiguity TREC Web queries: Q2E shows $-17.39$ Recall@100 vs. $-4.60$ for low-ambiguity
  - Popularity rank 3+: both BM25 and Contriever show degrading QE effectiveness
