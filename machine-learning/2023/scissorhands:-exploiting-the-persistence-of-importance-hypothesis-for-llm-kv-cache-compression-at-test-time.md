# Meta Information

- URL: [Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time](https://arxiv.org/abs/2305.17118)
- LICENSE: [arXiv.org - Non-exclusive license to distribute](https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html)
- Reference: Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali Shrivastava (2023). Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. NeurIPS 2023.

---

# Introduction

Large language models (LLMs) in autoregressive text generation maintain a **Key-Value (KV) cache** that stores intermediate attention computations for all previous tokens. This cache grows linearly with sequence length and batch size. At batch size 128 and sequence length 2048, the KV cache for OPT-175B reaches ~950 GB — approximately 3× the model weight size (~325 GB). This bottleneck limits both the maximum batch size (throughput) and the context length (capability).

Scissorhands addresses this by selectively discarding low-importance tokens from the KV cache **at test time**, without any fine-tuning or model weight changes.

**Applicability:** Inference-time deployment of large autoregressive transformers (e.g., OPT, GPT family) where KV cache memory is the limiting factor. Effective for high-throughput serving with long sequences.

---

# Background: KV Cache in Transformers

In a decoder-only transformer with $L$ layers and $H$ attention heads, at each generation step $t$ the attention output for token $t$ is:

$$\text{Attn}(q_t, K_{1:t}, V_{1:t}) = \text{softmax}\!\left(\frac{q_t K_{1:t}^\top}{\sqrt{d_k}}\right) V_{1:t}$$

where:
- $q_t \in \mathbb{R}^{d_k}$: query vector at step $t$
- $K_{1:t} \in \mathbb{R}^{t \times d_k}$: cached key matrix for all previous tokens
- $V_{1:t} \in \mathbb{R}^{t \times d_v}$: cached value matrix for all previous tokens

The KV cache memory at step $T$ for a model with hidden dimension $d$ is:

$$\text{Mem}_{\text{KV}} = 2 \cdot L \cdot H \cdot T \cdot d_k \cdot \text{bytes per element}$$

For OPT-175B with $L=96$, $H=96$, $d_k=128$, this grows to hundreds of GB at realistic batch/sequence sizes.

---

# The Persistence of Importance Hypothesis

## Empirical Observation

The authors examine attention scores across generation steps and find that:
- The set of tokens receiving high attention scores changes very little between consecutive steps.
- **Persistence ratio** = fraction of top-$k$ tokens at step $t$ that remain in the top-$k$ at step $t+1$.

Empirically across multiple OPT model scales, the persistence ratio exceeds **95%** in most transformer layers. This means the "important" tokens identified at one step remain important at the next step with high probability.

## Theorem 3.1 (Persistence Under MLP Dynamics)

Under the assumption that the MLP sub-layer's skip connection dominates (i.e., $\|h_{\text{skip}}\|_2 \gg \|h_{\text{MLP}}\|_2$, validated empirically by near-unit cosine similarity between input and output of each transformer block), the attention keys at step $t+1$ are approximately a linear transformation of the keys at step $t$:

$$k_{i}^{(t+1)} \approx k_{i}^{(t)} + \delta$$

where $\delta$ is a small perturbation from the MLP nonlinearity. Combined with the power-law distribution of attention scores, this implies that high-scoring tokens at step $t$ remain high-scoring at step $t+1$ with high probability.

---

# Scissorhands Algorithm

## Overview

Scissorhands maintains a fixed KV cache budget $B$ (maximum number of token slots). When the cache would exceed $B$, it removes tokens with the lowest cumulative importance scores. A **recent window** of size $r$ is always protected, because the importance of the most recent tokens is uncertain (not yet reflected in the history).

## Algorithm: Scissorhands KV Cache Management

```
Input:
  - KV cache C = [(k_1, v_1), ..., (k_t, v_t)]
  - Attention score accumulator S = [s_1, ..., s_t]   # sum of attn scores each token received
  - Budget B (max cache slots)
  - History window w (how many past steps to accumulate scores over)
  - Recent window r (number of most-recent tokens always kept)
  - Drop amount m (number of tokens to evict when over budget)

At each generation step t:
  1. Compute attention: a = softmax(q_t @ K^T / sqrt(d_k))   # shape: (t,)
  2. Update importance scores: S[i] += a[i]   for i in 1..t
     (maintain a sliding window of the last w steps; subtract old contributions)
  3. If len(C) > B:
       a. Protect the last r tokens (recent window)
       b. Among non-recent tokens, find m tokens with lowest S scores
       c. Evict those m tokens from C and S
  4. Generate output: o_t = a @ V   (using surviving cache)
  5. Append (k_t, v_t) to C; append a[t] to S
```

**Key parameters used in experiments:**
| Parameter | Value |
|-----------|-------|
| History window $w$ | 400 |
| Recent window $r$ | 10 |
| Drop amount $m$ | 0.5B (per batch step) |

**Input/Output:**
- Input to each attention layer: query $q_t \in \mathbb{R}^{d_k}$, compressed KV cache $C \subset \mathbb{R}^{B \times d_k} \times \mathbb{R}^{B \times d_v}$
- Output: attention result $o_t \in \mathbb{R}^{d_v}$

## Theorem 4.1 (Error Bound)

When Scissorhands drops tokens with low attention scores, the error in the generated output satisfies:

$$\|o_t^{\text{compressed}} - o_t^{\text{full}}\|_2 \leq \mathcal{O}\!\left(1 - \frac{B}{T_{\max}}\right)$$

where $T_{\max}$ is the full sequence length and $B$ is the cache budget. As $B \to T_{\max}$ the error vanishes; as $B$ decreases, the error scales with the compression ratio. The bound relies on the power-law concentration of attention scores, meaning most tokens receive negligible attention mass.

---

# Comparison with Related Methods

| Method | Requires Retraining | Memory Reduction | Mechanism |
|--------|-------------------|-----------------|-----------|
| **Scissorhands** | No | Up to 5× | Evict low-importance KV entries based on persistence |
| H2O (Heavy Hitter Oracle) | No | Moderate | Keep top-k "heavy hitter" tokens by cumulative score |
| StreamingLLM | No | High (fixed window) | Keep only initial + recent tokens; forgets middle context |
| Sliding Window Attention | Yes (at train time) | Moderate | Restrict attention to local window |
| Quantization (e.g., 4-bit KV) | No | ~4× | Reduce precision of cached values |

> [!IMPORTANT]
> Scissorhands is compatible with quantization. The paper demonstrates that combining 4-bit KV quantization with Scissorhands compression gives additive memory benefits without compounding errors.

**Difference from H2O:** H2O also selects important tokens by cumulative attention scores but does not model the persistence property or protect a recent window explicitly. Scissorhands's theoretical justification via Theorem 3.1 distinguishes it.

**Difference from StreamingLLM:** StreamingLLM always discards the middle of the context, retaining only a fixed initial window ("attention sinks") plus recent tokens. Scissorhands dynamically identifies which tokens are important, preserving mid-context pivotal tokens.

---

# Experiments

## Datasets and Tasks

| Dataset | Task | Metric |
|---------|------|--------|
| C4 | Language modeling (perplexity) | Perplexity ↓ |
| Winogrande | Commonsense reasoning (5-shot) | Accuracy ↑ |
| MathQA | Mathematical reasoning (5-shot) | Accuracy ↑ |

## Models

- OPT-6.7B, OPT-13B, OPT-30B, OPT-66B (Meta)
- Experiments at batch size 1 for quality evaluation; batch size scaling for throughput analysis

## Key Results

- **No perplexity degradation** on C4 until 5× compression for OPT-66B (i.e., retaining only 20% of KV cache).
- **5-shot accuracy preserved** on Winogrande and MathQA with 15–30% of the original KV cache retained.
- **Throughput:** Freed KV cache memory allows proportionally larger batch sizes, directly increasing tokens/second.
- **Quantization combination:** 4-bit KV + Scissorhands shows no additional error compared to each method alone.

## Hardware

- Not explicitly specified for all experiments; memory analysis references A100 GPU configurations.

---

# Practical Implications

Scissorhands is most useful for:
1. **High-throughput LLM serving** where batch size is limited by KV cache memory.
2. **Long-context inference** where storing all token KV pairs is infeasible.
3. **Edge/resource-constrained deployment** where model weights cannot be modified but inference memory must be reduced.

No gradient computation or model weight updates are needed; the compression runs entirely in the inference forward pass.

> [!NOTE]
> "The persistence ratio exceeds 95% in most transformer layers" — this is the empirical foundation that makes score-based eviction reliable without retraining.

> [!CAUTION]
> The theoretical analysis (Theorem 3.1) assumes skip connection dominance, which is validated empirically for OPT models. This assumption may not hold equally across all transformer architectures (e.g., those with stronger MLP nonlinearities or different normalization schemes).
