# Meta Information

- URL: [Scalable Causal Discovery with Score Matching](https://arxiv.org/abs/2304.03382)
- LICENSE: [arXiv.org - Non-exclusive license to distribute](https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html)
- Reference: Montagna, F., Noceti, N., Rosasco, L., Zhang, K., & Locatello, F. (2023). Scalable Causal Discovery with Score Matching. *Proceedings of the Second Conference on Causal Learning and Reasoning (CLeaR 2023)*, PMLR 213:752–771.

# Scalable Causal Discovery with Score Matching (DAS)

## Overview

This paper introduces **DAS (Discovery At Scale)**, a method for recovering the complete causal directed acyclic graph (DAG) from purely observational data generated by nonlinear additive Gaussian noise models (ANMs). The key insight is that the **second derivative (diagonal of the Hessian) of the log-likelihood** with respect to each variable provides information about the causal ordering of variables, and this can be approximated efficiently using score matching without training a full generative model.

**Applicability:** Researchers and practitioners who need to infer causal structure (not just correlation) from observational tabular data, especially in high-dimensional settings ($d \geq 100$ variables) where classical constraint-based (PC, FCI) or continuous-optimization-based (NOTEARS) methods are computationally prohibitive.

> [!IMPORTANT]
> DAS targets the **nonlinear additive Gaussian noise model** (ANM-G): $X_j = f_j(\mathbf{X}_{pa(j)}) + \varepsilon_j$, where $\varepsilon_j \sim \mathcal{N}(0, \sigma_j^2)$ and $f_j$ are arbitrary nonlinear functions. The Gaussian noise assumption is essential—the theorems do not hold for arbitrary noise distributions (see companion paper NoGAM for the general case).

## Problem Formulation

### Causal Model

Given $n$ i.i.d. observations $\mathbf{X} = \{x^{(i)}\}_{i=1}^n$ of $d$ variables, the goal is to recover the DAG $\mathcal{G} = (V, E)$ such that:

$$X_j = f_j(\mathbf{X}_{pa_{\mathcal{G}}(j)}) + \varepsilon_j, \quad \varepsilon_j \sim \mathcal{N}(0, \sigma_j^2)$$

where $pa_{\mathcal{G}}(j)$ denotes the parents of node $j$ in $\mathcal{G}$, and $f_j$ are unknown nonlinear functions. The model is identifiable: the true DAG is the unique DAG consistent with the joint distribution $p(\mathbf{X})$ under this model class.

### Challenges with Prior Approaches

| Approach | Representative Methods | Bottleneck |
|---|---|---|
| Constraint-based | PC, FCI | Requires many conditional independence tests; exponential in worst case |
| Score-based with DAG constraint | NOTEARS, DAG-GNN | DAG constraint enforcement has $O(d^3)$ per-iteration cost |
| Restricted functional models (ordering-based) | SCORE (Rolland et al.) | Recovers topological ordering only; separate expensive pruning step |

DAS addresses all three bottlenecks by combining score function estimation with an efficient pruning strategy.

## Mathematical Foundation

### Score Function

The **score function** of a distribution $p(\mathbf{X})$ is the gradient of its log-density:

$$s(\mathbf{x}) = \nabla_{\mathbf{x}} \log p(\mathbf{x}) \in \mathbb{R}^d$$

The **score Jacobian** (Hessian of $\log p$) is:

$$J_s(\mathbf{x}) = \nabla_{\mathbf{x}}^2 \log p(\mathbf{x}) \in \mathbb{R}^{d \times d}$$

where entry $(j, k)$ is $\frac{\partial^2 \log p(\mathbf{x})}{\partial x_j \partial x_k}$.

### Key Theorem: Diagonal Entries and Causal Ordering

Under the nonlinear ANM-G model, the **diagonal entries** of the score Jacobian satisfy:

$$\frac{\partial^2 \log p(\mathbf{x})}{\partial x_j^2} = -\frac{1}{\sigma_j^2} + \sum_{k \in ch(j)} \left(-\frac{(\partial_{x_j} f_k)^2}{\sigma_k^2} + \frac{\partial^2_{x_j} f_k}{\cdot}\right)$$

where $ch(j)$ denotes the children of node $j$ in the DAG. The critical consequence is:

> [!NOTE]
> **Leaf nodes** (nodes with no children) have the simplest diagonal score Jacobian entries: $\frac{\partial^2 \log p}{\partial x_j^2} = -\frac{1}{\sigma_j^2}$, which is **constant** with respect to $\mathbf{x}$. Non-leaf nodes receive additional variable-dependent contributions from their children's structural equations.

This means leaf nodes exhibit **zero variance** in their diagonal score Jacobian entries over the data distribution, while non-leaf nodes exhibit nonzero variance. DAS exploits this to identify topological order by iteratively removing leaves.

### Score Variance Criterion

Define the **score variance** of variable $j$ as:

$$v_j = \mathbb{V}_{\mathbf{x} \sim p}\left[\frac{\partial^2 \log p(\mathbf{x})}{\partial x_j^2}\right]$$

In a perfect setting, $v_j = 0$ iff $j$ is a leaf in the remaining subgraph. The topological ordering is recovered by:

1. Find $j^* = \arg\min_j v_j$ (the variable with minimum variance of diagonal Hessian entries)
2. Remove $j^*$ from the variable set (it is a leaf)
3. Repeat until all variables are ordered

## DAS Algorithm

### Phase 1: Topological Ordering via Score Matching

The diagonal of the score Jacobian is estimated without computing the full Hessian. DAS uses a **score network** $s_\theta(\mathbf{x}) \approx \nabla_{\mathbf{x}} \log p(\mathbf{x})$ trained via **sliced score matching** (SSM) or **denoising score matching** (DSM). The diagonal of the score Jacobian is then obtained by differentiating the score network output with respect to the input:

$$\hat{J}_{jj}(\mathbf{x}) = \frac{\partial [s_\theta(\mathbf{x})]_j}{\partial x_j}$$

This avoids computing all $d^2$ entries of the full Hessian; only the $d$ diagonal entries are needed.

**Algorithm: DAS Topological Ordering**

```
Input: Data X ∈ ℝ^{n×d}, score network s_θ
Output: Topological ordering π = [π_1, ..., π_d]

1. Train s_θ on X using score matching objective
2. Compute diagonal Hessian estimates:
   For each sample x^(i) and each variable j:
     ĥ_j(x^(i)) = ∂[s_θ(x^(i))]_j / ∂x_j
3. Compute empirical variance for each variable j:
   v̂_j = Var_{i}[ĥ_j(x^(i))]
4. remaining = {1, ..., d}
   For t = d, d-1, ..., 1:
     j* = argmin_{j ∈ remaining} v̂_j
     π_t = j*                    ← j* is the t-th in reverse order (leaf)
     remaining = remaining \ {j*}
     Re-estimate variances conditioning on removed variable
5. Return π
```

### Phase 2: Scalable Pruning

Given a topological ordering $\pi$, any DAG consistent with $\pi$ is a valid DAG. The set of candidate edges is all $\binom{d}{2}$ pairs $(j, k)$ where $\pi^{-1}(j) < \pi^{-1}(k)$ (i.e., $j$ precedes $k$ in ordering). Naïve pruning (fitting a regression for each node from all ancestors) requires $O(d^2)$ regressions.

**DAS Pruning Innovation:** Instead of testing all ancestor pairs, DAS uses the **off-diagonal entries** of the score Jacobian to identify actual parent-child relationships:

$$[J_s(\mathbf{x})]_{jk} = \frac{\partial^2 \log p(\mathbf{x})}{\partial x_j \partial x_k} \neq 0 \iff X_k \in pa(X_j) \text{ or } X_j \in pa(X_k)$$

Under the ANM-G model, the off-diagonal entry $[J_s]_{jk}$ is nonzero if and only if $X_j$ and $X_k$ are in a parent-child relationship (not merely ancestor-descendant). This allows DAS to:

1. Compute off-diagonal score Jacobian entries $\hat{J}_{jk}$ for all pairs $(j, k)$
2. Retain edge $(j \to k)$ only if $|\hat{J}_{jk}|$ exceeds a threshold and $j$ precedes $k$ in $\pi$
3. Run a final regression-based pruning **only on the candidate edges** identified in step 2

**Complexity Reduction:** Classical pruning requires $O(d)$ regressions each with up to $O(d)$ predictors, giving $O(d^2)$ total operations. DAS reduces this by a factor of the average in-degree $\bar{k}$, giving $O(d \cdot \bar{k})$ where $\bar{k} \ll d$ for sparse graphs.

> [!TIP]
> The score Jacobian off-diagonal sparsity pattern directly encodes the Markov blanket structure: $[J_s]_{jk} = 0$ if $X_j \perp\!\!\!\perp X_k \mid \mathbf{X}_{\text{rest}}$ under certain conditions. See [Hyvarinen (2005) Score Function](https://jmlr.org/papers/v6/hyvarinen05a.html) for the original score matching framework.

## Score Network Architecture and Training

- **Input:** $\mathbf{x} \in \mathbb{R}^d$
- **Output:** $s_\theta(\mathbf{x}) \in \mathbb{R}^d$ (approximation to $\nabla_{\mathbf{x}} \log p(\mathbf{x})$)
- **Architecture:** Multi-layer perceptron (MLP) with shared weights across variables
- **Training objective (Sliced Score Matching):**

$$\mathcal{L}_{SSM}(\theta) = \mathbb{E}_{\mathbf{x}, \mathbf{v}}\left[\mathbf{v}^\top J_{s_\theta}(\mathbf{x}) \mathbf{v} + \frac{1}{2}\|\mathbf{v}^\top s_\theta(\mathbf{x})\|^2\right]$$

where $\mathbf{v} \sim p_v$ is a random projection vector. In practice, DAS uses the **exact diagonal** $\frac{\partial [s_\theta]_j}{\partial x_j}$ (via automatic differentiation) rather than sliced approximations, for better accuracy.

- **Diagonal Hessian estimation:** Computed via automatic differentiation through the score network; $O(d)$ backward passes yield all diagonal entries.

## Comparison with Related Methods

| Method | Recovery Target | Key Limitation | Complexity |
|---|---|---|---|
| NOTEARS (Zheng et al., 2018) | Full DAG | DAG constraint: $O(d^3)$/iter | $O(d^3)$ per iteration |
| DAG-GNN (Yu et al., 2019) | Full DAG | Same DAG constraint bottleneck | $O(d^3)$ per iteration |
| CAM (Bühlmann et al., 2014) | Full DAG | Requires $O(d^2)$ regressions in pruning | $O(d^2)$ regressions |
| SCORE (Rolland et al., 2022) | Topological ordering only | No direct edge recovery; expensive post-hoc pruning | $O(d^2)$ for pruning |
| **DAS (this work)** | **Full DAG** | Requires Gaussian noise assumption | **$O(d \cdot \bar{k})$ for pruning** |

> [!IMPORTANT]
> DAS is strictly more expressive than SCORE: it recovers the complete DAG (not just ordering), while using the same score network. The speedup comes from replacing the naive $O(d^2)$ pruning with score Jacobian-guided pruning that scales linearly with the number of true edges.

## Experiments

### Datasets

All experiments use **synthetically generated** data from nonlinear additive Gaussian noise models:

- **Graph structures:** Erdős–Rényi (ER) graphs with expected degrees ER1 ($\bar{k}=1$, sparse) and ER4 ($\bar{k}=4$, denser)
- **Variable counts ($d$):** 10, 20, 50, 100, 200, 500 variables
- **Sample sizes ($n$):** 1,000–5,000 observations
- **Structural equations:** Random nonlinear functions (e.g., neural networks, GPs)
- **Noise:** Independent Gaussian noise $\varepsilon_j \sim \mathcal{N}(0, \sigma_j^2)$

No real-world benchmark datasets are used in the main experiments; the focus is on synthetic evaluation under controlled conditions.

### Hardware

Experiments run on standard GPU hardware; specific hardware not detailed in the abstract.

### Evaluation Metrics

- **Structural Hamming Distance (SHD):** Number of edge insertions, deletions, and reversals required to transform predicted DAG into true DAG (lower is better)
- **Structural Intervention Distance (SID):** Measures closeness of causal effects (lower is better)
- **Runtime (seconds):** Wall-clock time including score network training and pruning

### Key Results

- DAS achieves **competitive SHD and SID** compared to SCORE + pruning and other baselines across all graph sizes and densities
- DAS is **more than 10× faster** than SCORE with CAM pruning on graphs with $d \geq 100$ variables
- Speedup factor increases with $d$: at $d=500$, DAS is orders of magnitude faster than NOTEARS-family methods which fail to scale
- On sparse graphs (ER1), DAS's pruning advantage is largest since $\bar{k}$ is small relative to $d$

> [!CAUTION]
> The experimental evaluation uses only synthetic data with known ground-truth DAGs. Performance on real-world datasets (biology, economics) is not demonstrated, and violations of the Gaussian noise assumption in practice could degrade accuracy.

## Limitations and Assumptions

1. **Gaussian noise required:** The score variance criterion for ordering identification relies specifically on Gaussian $\varepsilon_j$. Non-Gaussian noise invalidates the theoretical guarantees (addressed in the companion paper NoGAM).
2. **Faithfulness assumption:** Standard causal discovery assumption that all conditional independencies in the distribution correspond to d-separations in the graph.
3. **Observational data only:** No interventional data used; confounders are assumed absent.
4. **Score network quality:** The ordering and pruning quality depend on how well the score network approximates $\nabla \log p$.
