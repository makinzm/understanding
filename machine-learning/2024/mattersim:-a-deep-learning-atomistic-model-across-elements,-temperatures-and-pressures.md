# Meta Information

- URL: [MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures](https://arxiv.org/abs/2405.04967)
- LICENSE: [arXiv.org - Non-exclusive license to distribute](https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html)
- Reference: Yang, H., Li, J., Hao, H., Lu, Z., Hu, C., Zhou, Y., et al. (2024). MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures. arXiv preprint arXiv:2405.04967.

# MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures

MatterSim is a universal machine learning interatomic potential (MLIP) developed by Microsoft Research AI for Science. It predicts energies, forces, and stresses for materials across the entire periodic table under extreme conditions spanning 0–5000 K and 0–1000 GPa. Unlike prior models trained primarily on relaxed ground-state configurations, MatterSim trains on ~17 million structures that include far-from-equilibrium configurations sampled at finite temperatures and pressures, achieving a 10-fold precision improvement over prior best-in-class models on off-equilibrium benchmarks.

**Target users**: Computational materials scientists and chemists who need to simulate materials behavior at extreme conditions without running expensive first-principles DFT calculations for every configuration.

## Problem and Motivation

Density functional theory (DFT) is the gold standard for computing materials properties but is computationally prohibitive for large-scale molecular dynamics (MD) simulations or structure searches spanning thousands of configurations. Existing MLIPs are limited in scope: they cover narrow chemical spaces or only near-equilibrium geometries. The challenge is to build a general-purpose model that:

1. Covers the full periodic table under physically realistic temperature and pressure ranges
2. Accurately captures off-equilibrium dynamics required for finite-temperature MD
3. Is customizable to specialized systems via active learning with minimal labeled data

## Data Construction: Two Explorers

The training data is generated by a dual-explorer active learning strategy that seeds the dataset with both equilibrium and off-equilibrium structures:

**Ground-State Explorer (GSE)**: Collects relaxed structures from public databases (Materials Project, Alexandria, ICSD) and uses uncertainty-based sampling to find under-represented chemical environments. Structures with high model uncertainty are submitted to DFT labeling.

**Off-Equilibrium Explorer (OEE)**: Runs MD simulations at systematically varied temperatures (0–5000 K) and pressures (0–1000 GPa) using the current model version. Configurations at intervals of 200 ps are selected based on ensemble disagreement, then submitted to DFT. This produces configurations that would never appear in a relaxation-only dataset.

> [!IMPORTANT]
> The OEE is the critical differentiator. Relaxation-only databases under-represent high-temperature and high-pressure atomic environments by 2–10× compared to MatterSim's training set. For noble gases, coverage is 10× lower in prior databases due to their absence from most crystal structure repositories.

### Dataset Statistics

| Source | Configurations | Description |
|--------|---------------|-------------|
| Materials Project | ~1.5M | Relaxation trajectories |
| Alexandria | ~4M | High-throughput DFT relaxations |
| ICSD | ~500K | Experimental crystal structures |
| Internal OEE | ~11M | Finite-temperature MD snapshots |
| **Total** | **~17M** | All labeled at GGA-PBE (+ Hubbard U) level |

All labels are computed at the GGA-PBE level of theory with Hubbard U corrections for transition metals, using VASP with PAW pseudopotentials.

## Model Architectures

MatterSim provides two interchangeable backbone architectures: M3GNet (efficient) and Graphormer (high-accuracy).

### M3GNet (Graph Neural Network)

M3GNet is a message-passing neural network that incorporates two-body (distance) and three-body (angular) interactions among atoms within a cutoff radius $r_c$.

**Input**: A periodic crystal structure represented as a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ where nodes $v_i \in \mathcal{V}$ are atoms and edges $e_{ij} \in \mathcal{E}$ connect atom pairs within $r_c$.

- Node features: $\mathbf{h}_i^{(0)} \in \mathbb{R}^{d_\text{node}}$ initialized from atomic number embeddings
- Edge features: $\mathbf{e}_{ij}^{(0)} \in \mathbb{R}^{d_\text{edge}}$ initialized using radial basis expansion

**Radial and Angular Encoding**: Edge features are augmented with angular information using spherical Bessel functions $j_\ell$ and spherical harmonics $Y_\ell^0$:

```math
\begin{align}
  \phi_{ij} = \sum_{k} j_\ell\!\left(\frac{z_{\ell n} \|\mathbf{r}_{ij}\|}{r_c}\right) Y_\ell^0(\theta_{jik})
\end{align}
```

where $\theta_{jik}$ is the bond angle at atom $i$ between neighbors $j$ and $k$, and $z_{\ell n}$ are the zeros of $j_\ell$.

**Smooth cutoff function**: All interactions are smoothly truncated to zero at $r_c$ using:

```math
\begin{align}
  f_c(r) = 1 - 6\left(\frac{r}{r_c}\right)^5 + 15\left(\frac{r}{r_c}\right)^4 - 10\left(\frac{r}{r_c}\right)^3
\end{align}
```

**Message passing**: After $L$ interaction layers, each node aggregates messages from its neighborhood. The total energy is the sum of per-atom energy contributions $\varepsilon_i$:

```math
\begin{align}
  E = \sum_{i} \varepsilon_i(\mathbf{h}_i^{(L)})
\end{align}
```

**Output**: Forces and stresses are derived via automatic differentiation (no separate output heads):

```math
\begin{align}
  \mathbf{f}_i &= -\frac{\partial E}{\partial \mathbf{r}_i} \\
  \boldsymbol{\sigma} &= \frac{1}{V} \frac{\partial E}{\partial \boldsymbol{\varepsilon}}
\end{align}
```

where $V$ is the cell volume and $\boldsymbol{\varepsilon}$ is the strain tensor.

**Model size**: Scales from 880K to 4.5M parameters as training set grows to 3M structures. Training stability limits further scaling.

### Graphormer (Transformer-based)

Graphormer extends the transformer architecture to periodic crystal graphs, achieving higher accuracy at the cost of greater compute (182M parameters).

**Architecture**:
- 24 multi-head attention layers, 32 attention heads per layer
- Hidden dimension $d = 768$
- Distance encoding via 128 Gaussian basis kernels $\tilde{\Phi}(\|\mathbf{r}_{ij}\|) \in \mathbb{R}^{128}$

**Attention bias** encodes pairwise distances into the attention matrix:

```math
\begin{align}
  \text{Bias}_{ij} = \text{Linear}(\tilde{\Phi}(\|\mathbf{r}_{ij}\|))
\end{align}
```

**Periodic boundary condition (PBC) adaptation**: Multi-graph construction extends the unit cell by replicating atoms up to a 20 Å cutoff, including periodic images. The initial equivariant vector feature $\mathbf{e}_i^{(0)} \in \mathbb{R}^3$ is initialized to be translation-invariant:

```math
\begin{align}
  \mathbf{e}_i^{(0)} = \sum_j m_{ij} \cdot \frac{\mathbf{r}_{ij}}{\|\mathbf{r}_{ij}\|} \cdot \tilde{\Phi}(\|\mathbf{r}_{ij}\|)
\end{align}
```

where $m_{ij}$ are learned scalar attention weights.

**SO(3)-equivariant decoder**: 10 GeoMFormer layers refine the equivariant stream. Force output for atom $i$:

```math
\begin{align}
  \mathbf{f}_i = \text{Linear}(\|\mathbf{e}_i^{(n_2)}\|)
\end{align}
```

where $\mathbf{e}_i^{(n_2)} \in \mathbb{R}^3$ is the equivariant vector after $n_2$ GeoMFormer layers.

**Stress head**: Uses normalized lattice vector outer products to construct the $3 \times 3$ stress tensor prediction.

### Architecture Comparison

| Feature | M3GNet | Graphormer |
|---------|--------|------------|
| Approach | Message-passing GNN | Transformer |
| Parameters | 880K–4.5M | 182M |
| Angular info | Spherical Bessel + harmonics | Distance bias only |
| Compute | Efficient (fast MD) | High (DFT-replacement) |
| Strength | Long simulations | Highest accuracy |

## Active Learning for Customization

MatterSim supports fine-tuning to specialized systems using far fewer labeled structures than training from scratch. The active learning loop:

1. **Generate candidate structures** using random structure search or MD with the current model
2. **Estimate uncertainty** via ensemble disagreement (multiple model instances predict; variance = uncertainty)
3. **Select high-uncertainty structures** above a threshold for DFT labeling
4. **Retrain** on the labeled batch and repeat until uncertainty falls below the threshold

**Pseudocode**:
```
Initialize model θ₀ (pretrained MatterSim)
Initialize unlabeled pool U = {s₁, s₂, ..., sₙ}
Initialize labeled dataset D = {}

repeat:
    For each s ∈ U:
        σ(s) = ensemble_variance(θ, s)    # uncertainty estimate
    Select batch B = {s : σ(s) > τ}
    Label B with DFT → B_labeled
    D ← D ∪ B_labeled
    θ ← finetune(θ, D)
until σ(s) < τ for all s ∈ U
```

> [!NOTE]
> For liquid water at hybrid functional (HSE06) level: requires only 3% of the labeled structures needed when training from scratch. For Li₂B₁₂H₁₂ (a complex ionic conductor): requires 15% of scratch-training data.

## Zero-Shot Capabilities

Without any fine-tuning, MatterSim supports the following downstream tasks by running MD simulations or geometry optimizations on top of its energy/force/stress outputs:

### Materials Discovery

Random structure search using MatterSim's energy model identified 1,974 new thermodynamically stable binary compounds not present in the Materials Project database. Evaluated on the MatBench Discovery benchmark, MatterSim achieves an F1 score of 0.83.

### Phonon Spectra

Harmonic phonon spectra are computed via the finite displacement method (using Phonopy): small atomic displacements generate the force-constant matrix, from which phonon dispersion and density of states are derived. MatterSim achieves 0.87 THz MAE on maximum phonon frequency against PhononDB references.

### Mechanical Properties

Elastic constants are computed from energy-strain curves using finite differences. Bulk modulus prediction achieves 2.47 GPa MAE against experimental values.

### Gibbs Free Energy

Quasi-harmonic approximation (QHA) combines zero-point energy with finite-temperature phonon contributions to compute Gibbs free energy $G(T, P)$. MatterSim achieves sub-10 meV/atom error against experimental $G(T)$ curves up to 1000 K.

### Molecular Dynamics

NVT and NPT MD simulations run using MatterSim as the force engine. Evaluated across 118 diverse material systems (bulk metals, semiconductors, MOFs, 2D materials, polymers, surfaces), >90% of simulations complete successfully at 0–5000 K without unphysical behavior.

## Direct Property Prediction

Beyond simulation-based properties, MatterSim can be fine-tuned end-to-end for direct structure-to-property mapping. Fine-tuning on the MatBench suite:

| Property | MatterSim (fine-tuned) | Prior specialized model |
|----------|------------------------|------------------------|
| Band gap (eV) | 0.1290 | 0.1559 |
| Shear modulus log(GVRH) | 0.0608 | 0.0670 |
| Dielectric constant | Competitive | Baseline |
| Phonon max frequency (cm⁻¹) | 26.02 | 28.76 |

> [!NOTE]
> Fine-tuning outperforms specialized models that were trained only on the MatBench task, demonstrating that the broad materials space coverage in MatterSim's pretraining provides useful inductive bias.

## Comparison with Related Models

| Model | Chemical Coverage | Temperature/Pressure | Parameters | Key Strength |
|-------|-------------------|----------------------|------------|--------------|
| MACE-MP-0 | Periodic table | Near-equilibrium | ~5M | Open-source, efficient |
| CHGNet | Periodic table | Near-equilibrium | ~400K | Includes magnetic moments |
| SevenNet | Periodic table | Near-equilibrium | ~1M | Speed-optimized |
| GNoME | 89 elements | Near-equilibrium | — | Large-scale discovery |
| **MatterSim** | Periodic table | 0–5000 K, 0–1000 GPa | 4.5M / 182M | Off-equilibrium coverage |

Key differentiator: prior models trained on relaxation trajectories have 2–10× fewer distinct atomic environments than MatterSim. High-temperature and high-pressure environments require off-equilibrium training data that relaxation databases systematically miss.

> [!CAUTION]
> MatterSim uses semi-local (GGA) DFT labels and a finite interaction cutoff. It may underperform for: (1) systems dominated by long-range van der Waals forces (e.g., molecular crystals, polymers); (2) systems requiring beyond-GGA accuracy (strongly correlated electrons); (3) surfaces and interfaces not represented in training data.

## Experiments

- **Datasets**:
  - ~17M structures labeled at GGA-PBE level (Materials Project, Alexandria, ICSD, internally generated MD snapshots)
  - PhononDB: phonon property validation (~1,000 materials)
  - MatBench Discovery: thermodynamic stability of ~250K candidate structures
  - Custom MD benchmark: 118 diverse material systems across 9 families (bulk, MOF, 2D, polymer, surface, etc.)
  - MatBench property prediction: band gap (4,604 structures), shear modulus (10,987), dielectric constant (4,764), phonon frequency (1,265)

- **Hardware**: Not explicitly stated (Microsoft Research compute cluster)

- **Optimizer**: Adam with cosine learning rate schedule; multi-stage training with progressively larger datasets

- **Results**:
  - Energy MAE (off-equilibrium MPF-TP test set): 36 meV/atom
  - Bulk modulus MAE: 2.47 GPa
  - Phonon max frequency MAE: 0.87 THz
  - MatBench Discovery F1: 0.83
  - MD success rate: >90% across all 118 test systems at 0–5000 K
  - Phase transition prediction: MgO B1→B2 at 584 GPa (experiment: 429–562 GPa)
  - Fine-tuning data efficiency: 3% of scratch-training data for liquid water at hybrid DFT level
